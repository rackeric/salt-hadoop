#####################################
# DEVELOPMENT EDITION
#####################################

# Hue configuration file
# ===================================
#
# For complete documentation about the contents of this file, run
#       $ <hue_root>/build/env/bin/hue config_help
#
# All .ini files under the current directory are treated equally.  Their
# contents are merged to form the Hue configuration, which can
# can be viewed on the Hue at
#       http://<hue_host>:<port>/dump_config


###########################################################################
# General configuration for core Desktop features (authentication, etc)
###########################################################################

[desktop]
  
  kredentials_dir="/tmp"

  send_dbug_messages=1

  # To show database transactions, set database_logging to 1
    database_logging=0

  # Set this to a random string, the longer the better.
    # This is used for secure hashing in the session store.
      secret_key=secretkeysecretkeysecretkeysecretkey

  # Webserver listens on this address and port
    http_host=0.0.0.0
      http_port=8000

  # Time zone name
    time_zone=America/Los_Angeles

  # Turn off debug
    django_debug_mode=1

  # Turn off backtrace for server error
    http_500_debug_mode=1

  # Server email for internal error messages
    ## django_server_email='hue@localhost.localdomain'

  # Email backend
    ## django_email_backend=django.core.mail.backends.smtp.EmailBackend

  # Set to true to use CherryPy as the webserver, set to false
    # to use Spawning as the webserver. Defaults to Spawning if
      # key is not specified.
        ## use_cherrypy_server=false

  # Webserver runs as this user
    server_user=hue
      server_group=hadoop

  # If set to false, runcpserver will not actually start the web server.
    # Used if Apache is being used as a WSGI container.
      ## enable_server=yes

  # Number of threads used by the CherryPy web server
    ## cherrypy_server_threads=10

  # Filename of SSL Certificate
    ## ssl_certificate=

  # Filename of SSL RSA Private Key
    ## ssl_private_key=

  # Default encoding for site data
    ## default_site_encoding=utf-8

  # Options for X_FRAME_OPTIONS header. Default is SAMEORIGIN
    ## x_frame_options='ALLOW-FROM url'

  # Administrators
    # ----------------
      [[django_admins]]
          ## [[[admin1]]]
              ## name=john
                  ## email=john@doe.com

  # UI customizations
    # -------------------
      [[custom]]

  # Top banner HTML code
    ## banner_top_html=

  # Configuration options for user authentication into the web application
    # ------------------------------------------------------------------------
      [[auth]]

    # Authentication backend. Common settings are:
        # - django.contrib.auth.backends.ModelBackend (entirely Django backend)
            # - desktop.auth.backend.AllowAllBackend (allows everyone)
                # - desktop.auth.backend.AllowFirstUserDjangoBackend
                    #     (Default. Relies on Django and user manager, after the first login)
                        # - desktop.auth.backend.LdapBackend
                            # - desktop.auth.backend.PamBackend
                                # - desktop.auth.backend.SpnegoDjangoBackend
                                    # - desktop.auth.backend.RemoteUserDjangoBackend
                                        backend=desktop.auth.backend.AllowFirstUserDjangoBackend

    ## pam_service=login

    # When using the desktop.auth.backend.RemoteUserDjangoBackend, this sets
        # the normalized name of the header that contains the remote user.
            # The HTTP header in the request is converted to a key by converting
                # all characters to uppercase, replacing any hyphens with underscores
                    # and adding an HTTP_ prefix to the name. So, for example, if the header
                        # is called Remote-User that would be configured as HTTP_REMOTE_USER
                            #
                                # Defaults to HTTP_REMOTE_USER
                                    ## remote_user_header=HTTP_REMOTE_USER

  # Configuration options for connecting to LDAP and Active Directory
    # -------------------------------------------------------------------
      [[ldap]]

  # The search base for finding users and groups
    ## base_dn="DC=mycompany,DC=com"

  # The NT domain to connect to (only for use with Active Directory)
    ## nt_domain=mycompany.com

  # URL of the LDAP server
    ## ldap_url=ldap://auth.mycompany.com

  # Path to certificate for authentication over TLS
    ## ldap_cert=

  # Distinguished name of the user to bind as -- not necessary if the LDAP server
    # supports anonymous searches
      ## bind_dn="CN=ServiceAccount,DC=mycompany,DC=com"

  # Password of the bind user -- not necessary if the LDAP server supports
    # anonymous searches
      ## bind_password=

  # Pattern for searching for usernames -- Use <username> for the parameter
    # For use when using LdapBackend for Hue authentication
      ## ldap_username_pattern="uid=<username>,ou=People,dc=mycompany,dc=com"

  # Create users in Hue when they try to login with their LDAP credentials
    # For use when using LdapBackend for Hue authentication
      ## create_users_on_login=true

      [[[users]]]

      # Base filter for searching for users
            ## user_filter="objectclass=*"

      # The username attribute in the LDAP schema
            ## user_name_attr=sAMAccountName

      [[[groups]]]

      # Base filter for searching for groups
            ## group_filter="objectclass=*"

      # The username attribute in the LDAP schema
            ## group_name_attr=cn

  # Configuration options for specifying the Desktop Database.  For more info,
    # see http://docs.djangoproject.com/en/1.1/ref/settings/#database-engine
      # ------------------------------------------------------------------------
        [[database]]
            engine=sqlite3
                name=/var/lib/hue/desktop.db
                    # Database engine is typically one of:
                        # postgresql_psycopg2, mysql, or sqlite3
                            #
                                # Note that for sqlite3, 'name', below is a filename;
                                    # for other backends, it is the database name.
                                        ## engine=sqlite3
                                            ## host=
                                                ## port=
                                                    ## user=
                                                        ## password=
                                                            ## name=


  # Configuration options for connecting to an external SMTP server
    # ------------------------------------------------------------------------
      [[smtp]]

    # The SMTP server information for email notification delivery
        host=localhost
            port=25
                user=
                    password=

    # Whether to use a TLS (secure) connection when talking to the SMTP server
        tls=no

    # Default email address to use for various automated notification from Hue
        ## default_from_email=hue@localhost


  # Configuration options for Kerberos integration for secured Hadoop clusters
    # ------------------------------------------------------------------------
      [[kerberos]]

    # Path to Hue's Kerberos keytab file
        ## hue_keytab=/etc/security/keytabs/hue.service.keytab

    # Kerberos principal name for Hue
        ## hue_principal=hue/IP

    # Path to kinit
        ## kinit_path=/usr/bin/kinit

    ## Frequency in seconds with which Hue will renew its keytab. Default 1h.
        ## reinit_frequency=3600

    ## Path to keep Kerberos credentials cached.
        ## ccache_path=/tmp/hue_krb5_ccache


###########################################################################
# Settings to configure your Hadoop cluster.
###########################################################################

[hadoop]

  # Configuration for HDFS NameNode
    # ------------------------------------------------------------------------
      [[hdfs_clusters]]

    [[[default]]]
          # Enter the filesystem uri
                fs_defaultfs=hdfs://{{ pillar['namenode_FQDN'] }}:8020

      # Use WebHdfs/HttpFs as the communication mechanism. To fallback to
            # using the Thrift plugin (used in Hue 1.x), this must be uncommented
                  # and explicitly set to the empty value.
                        webhdfs_url=http://{{ pillar['namenode_FQDN'] }}:50070/webhdfs/v1/

      ## security_enabled=true

      # Settings about this HDFS cluster. If you install HDFS in a
            # different location, you need to set the following.

      # Defaults to $HADOOP_HDFS_HOME or /usr/lib/hadoop-hdfs
            hadoop_hdfs_home=/usr/lib/hadoop/lib

      # Defaults to $HADOOP_BIN or /usr/bin/hadoop
            ## hadoop_bin=/usr/bin/hadoop

      # Defaults to $HADOOP_CONF_DIR or /etc/hadoop/conf
            ## hadoop_conf_dir=/etc/hadoop/conf

  # Configuration for MapReduce JobTracker
    # ------------------------------------------------------------------------
      [[mapred_clusters]]

    [[[default]]]
          # Enter the host on which you are running the Hadoop JobTracker
                jobtracker_host={{ pillar['namenode_FQDN'] }}
                      # The port where the JobTracker IPC listens on
                            jobtracker_port=50300
                                  # Thrift plug-in port for the JobTracker
                                        ## thrift_port=9290
                                              # Whether to submit jobs to this cluster
                                                    submit_to=true

      # Job tracker kerberos principal
            ## jt_kerberos_principal=jt

      ## security_enabled=true

      # Settings about this MR1 cluster. If you install MR1 in a
            # different location, you need to set the following.

      # Defaults to $HADOOP_MR1_HOME or /usr/lib/hadoop-0.20-mapreduce
            hadoop_mapred_home=/usr/lib/hadoop/lib

      # Defaults to $HADOOP_BIN or /usr/bin/hadoop
            ## hadoop_bin=/usr/bin/hadoop

      # Defaults to $HADOOP_CONF_DIR or /etc/hadoop/conf
            ## hadoop_conf_dir=/etc/hadoop/conf

  # Configuration for Yarn
    # ------------------------------------------------------------------------
      [[yarn_clusters]]

    ## [[[default]]]
          # Enter the host on which you are running the ResourceManager
                ## resourcemanager_host=localhost
                      # The port where the ResourceManager IPC listens on
                            ## resourcemanager_port=8032
                                  # Whether to submit jobs to this cluster
                                        ## submit_to=false

      ## security_enabled=false

      # Settings about this MR2 cluster. If you install MR2 in a
            # different location, you need to set the following.

      # Defaults to $HADOOP_MR2_HOME or /usr/lib/hadoop-mapreduce
            ## hadoop_mapred_home=/usr/lib/hadoop/lib

      # Defaults to $HADOOP_BIN or /usr/bin/hadoop
            ## hadoop_bin=/usr/bin/hadoop

      # Defaults to $HADOOP_CONF_DIR or /etc/hadoop/conf
            ## hadoop_conf_dir=/etc/hadoop/conf

      # URL of the ResourceManager API
            ## resourcemanager_api_url=http://localhost:8088

      # URL of the ProxyServer API
            ## proxy_api_url=http://localhost:8088

      # URL of the HistoryServer API
            ## history_server_api_url=http://localhost:19888

      # URL of the NodeManager API
            ## node_manager_api_url=http://localhost:8042


###########################################################################
# Settings to configure liboozie
###########################################################################

[liboozie]
  # The URL where the Oozie service runs on. This is required in order for
    # users to submit jobs.
      oozie_url=http://{{ pillar['namenode_FQDN'] }}:11000/oozie

  ## security_enabled=true

  # Location on HDFS where the workflows/coordinator are deployed when submitted.
    ## remote_deployement_dir=/user/hue/oozie/deployments


###########################################################################
# Settings to configure the Oozie app
###########################################################################

[oozie]
  # Location on local FS where the examples are stored.
    ## local_data_dir=..../examples

  # Location on local FS where the data for the examples is stored.
    ## sample_data_dir=...thirdparty/sample_data

  # Location on HDFS where the oozie examples and workflows are stored.
    ## remote_data_dir=/user/hue/oozie/workspaces

  # Share workflows and coordinators information with all users. If set to false,
    # they will be visible only to the owner and administrators.
      ## share_jobs=true

  # Maximum of Oozie workflows or coodinators to retrieve in one API call.
    ## oozie_jobs_count=100


###########################################################################
# Settings to configure Beeswax
###########################################################################

[beeswax]

  # Host where Beeswax server Thrift daemon is running.
    # If Kerberos security is enabled, the fully-qualified domain name (FQDN) is
      # required, even if the Thrift daemon is running on the same host as Hue.
        ## beeswax_server_host=<FQDN of Beeswax Server>

  # Port where Beeswax Thrift server runs on.
    ## beeswax_server_port=8002

  # Host where internal metastore Thrift daemon is running.
    ## beeswax_meta_server_host=localhost

  # Configure the port the internal metastore daemon runs on.
    # Used only if hive.metastore.local is true.
      ## beeswax_meta_server_port=8003

  # Hive home directory
    ## hive_home_dir=/usr/lib/hive

  # Hive configuration directory, where hive-site.xml is located
    ## hive_conf_dir=/etc/hive/conf

  # Timeout in seconds for thrift calls to beeswax service
    ## beeswax_server_conn_timeout=120

  # Timeout in seconds for thrift calls to the hive metastore
    ## metastore_conn_timeout=10

  # Maximum Java heapsize (in megabytes) used by Beeswax Server.
    # Note that the setting of HADOOP_HEAPSIZE in $HADOOP_CONF_DIR/hadoop-env.sh
      # may override this setting.
        ## beeswax_server_heapsize=1000

  # Share saved queries with all users. If set to false, saved queries are
    # visible only to the owner and administrators.
      ## share_saved_queries=true

  # The backend to contact for queries/metadata requests
    # Choices are 'beeswax' (default), 'hiveserver2'.
      ## server_interface=beeswax


###########################################################################
# Settings to configure Job Designer
###########################################################################

[jobsub]
  # Location on HDFS where the jobsub examples and templates are stored.
    ## remote_data_dir=/user/hue/jobsub

  # Location on local FS where examples and template are stored.
    ## local_data_dir=..../data

  # Location on local FS where sample data is stored
    ## sample_data_dir=...thirdparty/sample_data


###########################################################################
# Settings to configure Job Browser
###########################################################################

[jobbrowser]
  # Share submitted jobs information with all users. If set to false,
    # submitted jobs are visible only to the owner and administrators.
      ## share_jobs=true


###########################################################################
# Settings to configure the Shell application
###########################################################################

[shell]
  # The shell_buffer_amount specifies the number of bytes of output per shell
    # that the Shell app will keep in memory. If not specified, it defaults to
      # 524288 (512 MiB).
        ## shell_buffer_amount=100

  # If you run Hue against a Hadoop cluster with Kerberos security enabled, the
    # Shell app needs to acquire delegation tokens for the subprocesses to work
      # correctly. These delegation tokens are stored as temporary files in some
        # directory. You can configure this directory here. If not specified, it
          # defaults to /tmp/hue_delegation_tokens.
            ## shell_delegation_token_dir=/tmp/hue_delegation_tokens

  [[ shelltypes ]]

    # Define and configure a new shell type "flume"
        # ------------------------------------------------------------------------
            #[[[ flume ]]]
                #  nice_name="Flume Shell"
                    # command="/usr/bin/flume shell"
                        #  help="The command-line Flume client interface."

    #  [[[[ environment ]]]]
            # You can specify environment variables for the Flume shell
                    # in this section.

    # Define and configure a new shell type "pig"
        # ------------------------------------------------------------------------
            [[[ pig ]]]
                  nice_name="Pig Shell (Grunt)"
                        command="/usr/bin/pig -l /dev/null"
                              help="The command-line interpreter for Pig"

      [[[[ environment ]]]]
              # You can specify environment variables for the Pig shell
                      # in this section. Note that JAVA_HOME must be configured
                              # for the Pig shell to run.

        [[[[[ JAVA_HOME ]]]]]
                  value="/usr/jdk/jdk1.6.0_31/"

        [[[[[ PATH ]]]]]
                  value = "/usr/local/bin:/bin:/usr/bin"

    # Define and configure a new shell type "hbase"
        # ------------------------------------------------------------------------
            [[[ hbase ]]]
                  nice_name="HBase Shell"
                        command="/usr/bin/hbase shell"
                              help="The command-line HBase client interface."

      [[[[ environment ]]]]
              # You can configure environment variables for the HBase shell
                      # in this section.

    # Define and configure a new shell type "sqoop2"
        # ------------------------------------------------------------------------
            #[[[ r_shell ]]]
                #  nice_name="R shell"
                    #  command="/usr/bin/R"
                        #  help="The R language for Statistical Computing"

      #[[[[ environment ]]]]
            #   You can configure environment variables for the Sqoop2 shell
                  #   in this section.
                        #   [[[[[ JAVA_HOME ]]]]]
                              #    value="/usr/jdk/jdk1.6.0_31/"

    # Define and configure a new shell type "bash" for testing only
        # ------------------------------------------------------------------------
            [[[ bash ]]]
                  nice_name="Bash (Test only!!!)"
                        command="/bin/bash"
                              help="A shell that does not depend on Hadoop components"


###########################################################################
# Settings for the User Admin application
###########################################################################

[useradmin]
  # The name of the default user group that users will be a member of
    default_user_group=hadoop

[hcatalog]
 templeton_url="http://{{ pillar['namenode_FQDN'] }}:50111/templeton/v1/"
  ## security_enabled=true

[about]
  tutorials_installed=false

[pig]
  udf_path="/tmp/udfs"

[proxy]
  whitelist="({{ pillar['namenode_FQDN'] }}|localhost|127\.0\.0\.1):(50030|50070|50060|50075|50111)",
